{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model train\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.functional import F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from models import *\n",
    "import tqdm\n",
    "from umap import UMAP\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read, understand and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_data = pd.read_parquet('data/amazon_product_data_word2vec.parquet')\n",
    "data = torch.load('data/amazon_product_data_concat.pt')\n",
    "data.num_classes = data.y.unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data Data(x=[729819, 1200], edge_index=[2, 680548], y=[729819], num_classes=10)\n",
      "num nodes 729819\n",
      "Num edges 680548\n",
      "num node features 1200\n",
      "is undirected False\n",
      "is directed True\n",
      "num edge features 0\n",
      "num classes 10\n"
     ]
    }
   ],
   "source": [
    "print(\"data\", data)\n",
    "print(\"num nodes\", data.num_nodes)\n",
    "print(\"Num edges\", data.num_edges)\n",
    "print(\"num node features\", data.num_node_features)\n",
    "print(\"is undirected\", data.is_undirected())\n",
    "print(\"is directed\", data.is_directed())\n",
    "print(\"num edge features\", data.num_edge_features)\n",
    "print('num classes', data.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_counts = node_data['main_category'].value_counts()\n",
    "\n",
    "# # plot a bar chart of the main categories\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.bar(value_counts.index, value_counts.values)\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.title('Main Category Distribution')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(h, color, epoch=None, loss=None):\n",
    "    z = UMAP(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    title_text = 'Epoch: {}'.format(epoch) if epoch is not None else ''\n",
    "    loss_text = 'Loss: {:.4f}'.format(loss) if loss is not None else ''\n",
    "    plt.title('Embedding visualization' + ' ' + title_text + ' ' + loss_text)\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.utils import to_networkx\n",
    "\n",
    "# G = to_networkx(data, to_undirected=True)\n",
    "# visualize_graph(G, color=data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(data, train_ratio, val_ratio):\n",
    "  num_nodes = data.num_nodes\n",
    "  indices = list(range(num_nodes))\n",
    "  np.random.shuffle(indices)\n",
    "  \n",
    "  # Create masks\n",
    "  train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "  val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "  test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "  \n",
    "  # Set proportions for train, val, and test\n",
    "  train_end = int(train_ratio * num_nodes)\n",
    "  val_end = train_end + int(val_ratio * num_nodes)\n",
    "  train_mask[indices[:train_end]] = True\n",
    "  val_mask[indices[train_end:val_end]] = True\n",
    "  test_mask[indices[val_end:]] = True\n",
    "  \n",
    "  # Assign the custom masks to the dataset\n",
    "  data.train_mask = train_mask\n",
    "  data.val_mask = val_mask\n",
    "  data.test_mask = test_mask\n",
    "\n",
    "\n",
    "def train_test_split_graph(data: Data, train_ratio: float, val_ratio: float, batch_size: int):\n",
    "    \"\"\"\n",
    "    Split the graph data into train, validation, and test sets\n",
    "    :param data: The graph data\n",
    "    :param train_ratio: The ratio of the training set\n",
    "    :param val_ratio: The ratio of the validation set\n",
    "    :param batch_size: The batch size\n",
    "    \n",
    "    :return: The train, validation, and test data loaders\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the function to create masks\n",
    "    create_masks(data, train_ratio, val_ratio)\n",
    "\n",
    "    # train, validation, and test node indices based on the masks\n",
    "    train_idx = data.train_mask.nonzero(as_tuple=False).view(-1)\n",
    "    val_idx = data.val_mask.nonzero(as_tuple=False).view(-1)\n",
    "    test_idx = data.test_mask.nonzero(as_tuple=False).view(-1)\n",
    "    \n",
    "    def create_data_loader(data, indices):\n",
    "        return DataLoader(data[indices], batch_size=batch_size, shuffle=True)\n",
    "      \n",
    "    def create_neighbor_loader(data, indices, batch_size=batch_size):\n",
    "        return NeighborLoader(data, num_neighbors=[30] * 2, batch_size=batch_size, input_nodes=indices)\n",
    "\n",
    "    # create the data loaders\n",
    "    train_loader = create_neighbor_loader(data, train_idx)\n",
    "    val_loader = create_neighbor_loader(data, val_idx)\n",
    "    test_loader = create_neighbor_loader(data, test_idx)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Training epoch\n",
    "def train_epoch(model, optimizer, loss_fn, train_loader: NeighborLoader, device, print_every, epoch=0):\n",
    "    model.train()\n",
    "    b_losses = []\n",
    "    b_accuracies = []\n",
    "    b_f1_scores = []\n",
    "    b_balanced_accuracies = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        out, h = model(batch.x, batch.edge_index)\n",
    "        loss = loss_fn(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        b_losses.append(loss.item())\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        labels = batch.y.cpu().numpy()\n",
    "        b_acc = (preds == labels).sum() / len(labels)\n",
    "        b_accuracies.append(b_acc)\n",
    "        \n",
    "        # Compute F1-score and balanced accuracy\n",
    "        b_f1 = f1_score(labels, preds, average='weighted')\n",
    "        b_bal_acc = balanced_accuracy_score(labels, preds)\n",
    "        b_f1_scores.append(b_f1)\n",
    "        b_balanced_accuracies.append(b_bal_acc)\n",
    "    \n",
    "    visualize(h, batch.y, epoch, loss=np.mean(b_losses))\n",
    "     \n",
    "    \n",
    "    return (\n",
    "        np.mean(b_losses),\n",
    "        np.mean(b_accuracies),\n",
    "        np.mean(b_f1_scores),\n",
    "        np.mean(b_balanced_accuracies),\n",
    "    )\n",
    "\n",
    "# Validation\n",
    "@torch.no_grad()\n",
    "def validate(model, loss_fn, val_loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in val_loader:\n",
    "        batch = batch.to(device)\n",
    "        out, _ = model(batch.x, batch.edge_index)\n",
    "        loss = loss_fn(out, batch.y)\n",
    "        total_loss += loss.item()\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        labels = batch.y.cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "        total_correct += (preds == labels).sum()\n",
    "        total_samples += len(labels)\n",
    "\n",
    "    val_acc = total_correct / total_samples\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    val_bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return total_loss / len(val_loader), val_acc, val_f1, val_bal_acc\n",
    "\n",
    "# Training loop\n",
    "def training_loop(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, train_accs, train_f1s, train_bal_accs = [], [], [], []\n",
    "    val_losses, val_accs, val_f1s, val_bal_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc, train_f1, train_bal_acc = train_epoch(\n",
    "            model, optimizer, loss_fn, train_loader, device, print_every, epoch\n",
    "        )\n",
    "        val_loss, val_acc, val_f1, val_bal_acc = validate(model, loss_fn, val_loader, device)\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{num_epochs}: \"\n",
    "            f\"Train loss: {train_loss:.3f}, Train acc.: {train_acc:.3f}, Train F1: {train_f1:.3f}, Train Bal. Acc.: {train_bal_acc:.3f}, \"\n",
    "            f\"Val. loss: {val_loss:.3f}, Val. acc.: {val_acc:.3f}, Val. F1: {val_f1:.3f}, Val. Bal. Acc.: {val_bal_acc:.3f}\"\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_f1s.append(train_f1)\n",
    "        train_bal_accs.append(train_bal_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "        val_bal_accs.append(val_bal_acc)\n",
    "\n",
    "    return model, train_losses, train_accs, train_f1s, train_bal_accs, val_losses, val_accs, val_f1s, val_bal_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "visualize() got an unexpected keyword argument 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     21\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m model, train_losses, train_accs, val_losses, val_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 81\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every)\u001b[0m\n\u001b[1;32m     78\u001b[0m val_losses, val_accs, val_f1s, val_bal_accs \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 81\u001b[0m     train_loss, train_acc, train_f1, train_bal_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     val_loss, val_acc, val_f1, val_bal_acc \u001b[38;5;241m=\u001b[39m validate(model, loss_fn, val_loader, device)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Bal. Acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_bal_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal. loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val. acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val. F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val. Bal. Acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_bal_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[30], line 34\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, loss_fn, train_loader, device, print_every, epoch)\u001b[0m\n\u001b[1;32m     31\u001b[0m     b_f1_scores\u001b[38;5;241m.\u001b[39mappend(b_f1)\n\u001b[1;32m     32\u001b[0m     b_balanced_accuracies\u001b[38;5;241m.\u001b[39mappend(b_bal_acc)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_losses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m     np\u001b[38;5;241m.\u001b[39mmean(b_losses),\n\u001b[1;32m     39\u001b[0m     np\u001b[38;5;241m.\u001b[39mmean(b_accuracies),\n\u001b[1;32m     40\u001b[0m     np\u001b[38;5;241m.\u001b[39mmean(b_f1_scores),\n\u001b[1;32m     41\u001b[0m     np\u001b[38;5;241m.\u001b[39mmean(b_balanced_accuracies),\n\u001b[1;32m     42\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: visualize() got an unexpected keyword argument 'loss'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = train_test_split_graph(data, train_ratio, val_ratio, batch_size)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "# model = GAT(in_channels=data.num_node_features, hidden_channels=64, out_channels=data.num_classes, num_heads=8).to(device)\n",
    "model = GCN(in_channels=data.num_node_features, hidden_channels=64, out_channels=data.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model, train_losses, train_accs, val_losses, val_accs = training_loop(\n",
    "  model, optimizer, loss_fn, train_loader, val_loader, num_epochs=10, print_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rigged",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
