{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model train\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/anaconda3/envs/rigged/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Reddit, Amazon\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.functional import F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from models import *\n",
    "import tqdm\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read, understand and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_data = pd.read_parquet('data/amazon_product_data_word2vec.parquet')\n",
    "data = torch.load('data/amazon_product_data_concat.pt')\n",
    "data.num_classes = data.y.unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data Data(x=[729819, 1200], edge_index=[2, 680548], y=[729819], num_classes=10)\n",
      "num nodes 729819\n",
      "Num edges 680548\n",
      "num node features 1200\n",
      "is undirected False\n",
      "is directed True\n",
      "num edge features 0\n",
      "num classes 10\n"
     ]
    }
   ],
   "source": [
    "print(\"data\", data)\n",
    "print(\"num nodes\", data.num_nodes)\n",
    "print(\"Num edges\", data.num_edges)\n",
    "print(\"num node features\", data.num_node_features)\n",
    "print(\"is undirected\", data.is_undirected())\n",
    "print(\"is directed\", data.is_directed())\n",
    "print(\"num edge features\", data.num_edge_features)\n",
    "print('num classes', data.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_counts = node_data['main_category'].value_counts()\n",
    "\n",
    "# # plot a bar chart of the main categories\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.bar(value_counts.index, value_counts.values)\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.title('Main Category Distribution')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.utils import to_networkx\n",
    "\n",
    "# G = to_networkx(data, to_undirected=True)\n",
    "# visualize_graph(G, color=data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_data_split_masks(data, train_ratio=0.8, val_ratio=0.1):\n",
    "#   # Total number of nodes\n",
    "#   num_nodes = data.num_nodes\n",
    "\n",
    "#   # Randomly permute the node indices\n",
    "#   perm = torch.randperm(num_nodes)\n",
    "\n",
    "#   # Calculate split sizes\n",
    "#   train_size = int(train_ratio * num_nodes)\n",
    "#   val_size = int(val_ratio * num_nodes)\n",
    "#   test_size = num_nodes - train_size - val_size\n",
    "\n",
    "#   # Create masks for train, validation, and test\n",
    "#   train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "#   val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "#   test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "#   # Assign masks\n",
    "#   train_mask[perm[:train_size]] = True\n",
    "#   val_mask[perm[train_size:train_size + val_size]] = True\n",
    "#   test_mask[perm[train_size + val_size:]] = True\n",
    "\n",
    "#   # Assign masks to the data object\n",
    "#   data.train_mask = train_mask\n",
    "#   data.val_mask = val_mask\n",
    "#   data.test_mask = test_mask\n",
    "  \n",
    "#   return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(data, train_ratio, val_ratio):\n",
    "  num_nodes = data.num_nodes\n",
    "  indices = list(range(num_nodes))\n",
    "  np.random.shuffle(indices)\n",
    "  \n",
    "  # Create masks\n",
    "  train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "  val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "  test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "  \n",
    "  # Set proportions for train, val, and test\n",
    "  train_end = int(train_ratio * num_nodes)\n",
    "  val_end = train_end + int(val_ratio * num_nodes)\n",
    "  train_mask[indices[:train_end]] = True\n",
    "  val_mask[indices[train_end:val_end]] = True\n",
    "  test_mask[indices[val_end:]] = True\n",
    "  \n",
    "  # Assign the custom masks to the dataset\n",
    "  data.train_mask = train_mask\n",
    "  data.val_mask = val_mask\n",
    "  data.test_mask = test_mask\n",
    "\n",
    "\n",
    "def train_test_split_graph(data: Data, train_ratio: float, val_ratio: float, batch_size: int):\n",
    "    \"\"\"\n",
    "    Split the graph data into train, validation, and test sets\n",
    "    :param data: The graph data\n",
    "    :param train_ratio: The ratio of the training set\n",
    "    :param val_ratio: The ratio of the validation set\n",
    "    :param batch_size: The batch size\n",
    "    \n",
    "    :return: The train, validation, and test data loaders\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the function to create masks\n",
    "    create_masks(data, train_ratio, val_ratio)\n",
    "\n",
    "    # train, validation, and test node indices based on the masks\n",
    "    train_idx = data.train_mask.nonzero(as_tuple=False).view(-1)\n",
    "    val_idx = data.val_mask.nonzero(as_tuple=False).view(-1)\n",
    "    test_idx = data.test_mask.nonzero(as_tuple=False).view(-1)\n",
    "    \n",
    "    def create_data_loader(data, indices):\n",
    "        return DataLoader(data[indices], batch_size=batch_size, shuffle=True)\n",
    "      \n",
    "    def create_neighbor_loader(data, indices, batch_size=batch_size):\n",
    "        return NeighborLoader(data, num_neighbors=[30] * 2, batch_size=batch_size, input_nodes=indices)\n",
    "\n",
    "    # create the data loaders\n",
    "    train_loader = create_neighbor_loader(data, train_idx)\n",
    "    val_loader = create_neighbor_loader(data, val_idx)\n",
    "    test_loader = create_neighbor_loader(data, test_idx)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epoch(model, optimizer, loader, device):\n",
    "#     \"\"\"Train the model for one epoch using NeighborSampler mini-batches.\"\"\"\n",
    "#     model.train()\n",
    "#     b_losses = []\n",
    "#     b_accuracies = []\n",
    "\n",
    "#     for batch in loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         batch.to(device)\n",
    "        \n",
    "#         out = model(batch.x, batch.edge_index)\n",
    "#         loss = F.nll_loss(out, batch.y)\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         b_losses.append(loss.item())\n",
    "#         b_acc = (out.argmax(dim=1) == batch.y).sum().item() / batch.y.size(0)\n",
    "#         b_accuracies.append(b_acc)\n",
    "        \n",
    "#     return np.mean(b_losses), np.mean(b_accuracies)\n",
    "\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate(model, data_loader, device, data):\n",
    "#     \"\"\"Evaluate the model on the validation or test set using NeighborSampler mini-batches.\"\"\"\n",
    "#     model.eval()\n",
    "#     total_correct = 0\n",
    "\n",
    "#     for batch_size, n_id, adjs in data_loader:\n",
    "#         adjs = [adj.to(device) for adj in adjs]\n",
    "\n",
    "#         # Compute predictions for all nodes in the mini-batch\n",
    "#         x_input = data.x[n_id].to(device)\n",
    "#         out = model(x_input, adjs[0].edge_index)\n",
    "\n",
    "#         # Get predictions only for the first `batch_size` target nodes\n",
    "#         pred = out[:batch_size].max(dim=1)[1]\n",
    "\n",
    "#         # Compare with the actual labels of the target nodes\n",
    "#         total_correct += (pred ==\n",
    "#                           data.y[n_id[:batch_size]].to(device)).sum().item()\n",
    "\n",
    "#     # Calculate accuracy based on the number of target nodes in the entire set\n",
    "#     return total_correct / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Training epoch\n",
    "def train_epoch(model, optimizer, loss_fn, train_loader, device, print_every):\n",
    "    model.train()\n",
    "    b_losses = []\n",
    "    b_accuracies = []\n",
    "    b_f1_scores = []\n",
    "    b_balanced_accuracies = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = loss_fn(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        b_losses.append(loss.item())\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        labels = batch.y.cpu().numpy()\n",
    "        b_acc = (preds == labels).sum() / len(labels)\n",
    "        b_accuracies.append(b_acc)\n",
    "        \n",
    "        # Compute F1-score and balanced accuracy\n",
    "        b_f1 = f1_score(labels, preds, average='weighted')\n",
    "        b_bal_acc = balanced_accuracy_score(labels, preds)\n",
    "        b_f1_scores.append(b_f1)\n",
    "        b_balanced_accuracies.append(b_bal_acc)\n",
    "\n",
    "    return (\n",
    "        np.mean(b_losses),\n",
    "        np.mean(b_accuracies),\n",
    "        np.mean(b_f1_scores),\n",
    "        np.mean(b_balanced_accuracies),\n",
    "    )\n",
    "\n",
    "# Validation\n",
    "@torch.no_grad()\n",
    "def validate(model, loss_fn, val_loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in val_loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = loss_fn(out, batch.y)\n",
    "        total_loss += loss.item()\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        labels = batch.y.cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "        total_correct += (preds == labels).sum()\n",
    "        total_samples += len(labels)\n",
    "\n",
    "    val_acc = total_correct / total_samples\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    val_bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return total_loss / len(val_loader), val_acc, val_f1, val_bal_acc\n",
    "\n",
    "# Training loop\n",
    "def training_loop(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, train_accs, train_f1s, train_bal_accs = [], [], [], []\n",
    "    val_losses, val_accs, val_f1s, val_bal_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc, train_f1, train_bal_acc = train_epoch(\n",
    "            model, optimizer, loss_fn, train_loader, device, print_every\n",
    "        )\n",
    "        val_loss, val_acc, val_f1, val_bal_acc = validate(model, loss_fn, val_loader, device)\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{num_epochs}: \"\n",
    "            f\"Train loss: {train_loss:.3f}, Train acc.: {train_acc:.3f}, Train F1: {train_f1:.3f}, Train Bal. Acc.: {train_bal_acc:.3f}, \"\n",
    "            f\"Val. loss: {val_loss:.3f}, Val. acc.: {val_acc:.3f}, Val. F1: {val_f1:.3f}, Val. Bal. Acc.: {val_bal_acc:.3f}\"\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_f1s.append(train_f1)\n",
    "        train_bal_accs.append(train_bal_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "        val_bal_accs.append(val_bal_acc)\n",
    "\n",
    "    return model, train_losses, train_accs, train_f1s, train_bal_accs, val_losses, val_accs, val_f1s, val_bal_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1/10: Train loss: 0.178, Train acc.: 0.948, Train F1: 0.945, Train Bal. Acc.: 0.909, Val. loss: 0.050, Val. acc.: 0.986, Val. F1: 0.986, Val. Bal. Acc.: 0.985\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = train_test_split_graph(data, train_ratio, val_ratio, batch_size)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = GAT(in_channels=data.num_node_features, hidden_channels=64, out_channels=data.num_classes, num_heads=8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model, train_losses, train_accs, val_losses, val_accs = training_loop(\n",
    "  model, optimizer, loss_fn, train_loader, val_loader, num_epochs=10, print_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rigged",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
