{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/anaconda3/envs/rigged/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#from torch_geometric.datasets import Reddit, Amazon\n",
    "#from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "#import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "#from torch.functional import F\n",
    "#rom torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.data import Data\n",
    "#import torch.optim as optim\n",
    "#import seaborn as sns\n",
    "from baseline_models import *\n",
    "#import tqdm\n",
    "#from sklearn.manifold import TSNE\n",
    "#from umap import UMAP\n",
    "#import logging\n",
    "#import concurrent\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data` is loaded with the node features (x) and labels (y)\n",
    "data = torch.load('data/amazon_product_data_sum.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract node features and labels\n",
    "node_features = data.x\n",
    "node_labels = data.y\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(node_features, node_labels, test_size=0.2, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create a TensorDataset for each dataset\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "# Create a DataLoader for the training, validation, and test node sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    correct = (predictions.argmax(dim=1) == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def f1(predictions, labels):\n",
    "    preds = predictions.argmax(dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    return f1_score(labels, preds, average='weighted')\n",
    "  \n",
    "def balanced_accuracy(predictions, labels):\n",
    "    preds = predictions.argmax(dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    return balanced_accuracy_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training and validation loops (as before, without edge_index)\n",
    "\n",
    "def train_epoch(model, optimizer, loss_fn, train_loader, device, metrics):\n",
    "    model.train()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    losses = []\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        out = model(x_batch)\n",
    "        loss = loss_fn(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        all_preds.append(out.detach().cpu())\n",
    "        all_labels.append(y_batch.detach().cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    avg_metrics = {metric_name: metric_fn(all_preds, all_labels) for metric_name, metric_fn in metrics.items()}\n",
    "    avg_loss = np.mean(losses)\n",
    "\n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loss_fn, val_loader, device, metrics):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    losses = []\n",
    "\n",
    "    for x_batch, y_batch in val_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        loss = loss_fn(out, y_batch)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        all_preds.append(out.detach().cpu())\n",
    "        all_labels.append(y_batch.detach().cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    avg_metrics = {metric_name: metric_fn(all_preds, all_labels) for metric_name, metric_fn in metrics.items()}\n",
    "    avg_loss = np.mean(losses)\n",
    "\n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "# Training loop remains unchanged (just ensure you are passing the right DataLoader now)\n",
    "\n",
    "def training_loop(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, device, metrics):\n",
    "    print(\"Starting training\")\n",
    "    train_losses, val_losses = [], []\n",
    "    train_metrics_history = {metric_name: [] for metric_name in metrics}\n",
    "    val_metrics_history = {metric_name: [] for metric_name in metrics}\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Training\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, loss_fn, train_loader, device, metrics)\n",
    "        # Validation\n",
    "        val_loss, val_metrics = validate(model, loss_fn, val_loader, device, metrics)\n",
    "        \n",
    "        # Logging results\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        for metric_name in metrics:\n",
    "            train_metrics_history[metric_name].append(train_metrics[metric_name])\n",
    "            val_metrics_history[metric_name].append(val_metrics[metric_name])\n",
    "\n",
    "        metrics_str = ', '.join(\n",
    "            [f'{metric_name}: {train_metrics[metric_name]:.3f} (train), {val_metrics[metric_name]:.3f} (val)'\n",
    "             for metric_name in metrics])\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{num_epochs}: \"\n",
    "            f\"Loss: {train_loss:.3f} (train), {val_loss:.3f} (val), \"\n",
    "            f\"{metrics_str}\"\n",
    "        )\n",
    "\n",
    "    return model, train_losses, val_losses, train_metrics_history, val_metrics_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x_train = x_train.to(device)\n",
    "x_val = x_val.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_val = y_val.to(device)\n",
    "y_test = y_test.to(device)\n",
    "learning_rate = 0.01  \n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model, train_loader, val_loader):\n",
    "  print(model.__class__.__name__)\n",
    "  \n",
    "  optimizer = torch.optim.Adam(\n",
    "      model.parameters(), lr=learning_rate)\n",
    "  \n",
    "  loss_fn = torch.nn.CrossEntropyLoss()\n",
    "  \n",
    "  metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'balanced_accuracy': balanced_accuracy,\n",
    "  }\n",
    "  \n",
    "  # Train the model\n",
    "  model, train_losses, val_losses, train_metrics_history, val_metrics_history = training_loop(\n",
    "    model, optimizer, loss_fn, train_loader, val_loader, num_epochs=10, device=device, metrics=metrics\n",
    "  )\n",
    "  \n",
    "  return {\n",
    "    'model': model,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_metrics_history': train_metrics_history,\n",
    "    'val_metrics_history': val_metrics_history\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "Starting training\n",
      "Epoch 1/10: Loss: 0.646 (train), 0.480 (val), accuracy: 0.793 (train), 0.848 (val), f1: 0.792 (train), 0.847 (val), balanced_accuracy: 0.787 (train), 0.848 (val)\n",
      "Epoch 2/10: Loss: 0.402 (train), 0.347 (val), accuracy: 0.875 (train), 0.891 (val), f1: 0.874 (train), 0.891 (val), balanced_accuracy: 0.872 (train), 0.884 (val)\n",
      "Epoch 3/10: Loss: 0.312 (train), 0.298 (val), accuracy: 0.903 (train), 0.906 (val), f1: 0.903 (train), 0.906 (val), balanced_accuracy: 0.900 (train), 0.904 (val)\n",
      "Epoch 4/10: Loss: 0.275 (train), 0.281 (val), accuracy: 0.915 (train), 0.913 (val), f1: 0.915 (train), 0.913 (val), balanced_accuracy: 0.912 (train), 0.904 (val)\n",
      "Epoch 5/10: Loss: 0.253 (train), 0.275 (val), accuracy: 0.922 (train), 0.915 (val), f1: 0.922 (train), 0.915 (val), balanced_accuracy: 0.919 (train), 0.913 (val)\n",
      "Epoch 6/10: Loss: 0.238 (train), 0.245 (val), accuracy: 0.926 (train), 0.924 (val), f1: 0.926 (train), 0.924 (val), balanced_accuracy: 0.923 (train), 0.920 (val)\n",
      "Epoch 7/10: Loss: 0.229 (train), 0.242 (val), accuracy: 0.929 (train), 0.924 (val), f1: 0.929 (train), 0.925 (val), balanced_accuracy: 0.926 (train), 0.922 (val)\n",
      "Epoch 8/10: Loss: 0.221 (train), 0.229 (val), accuracy: 0.932 (train), 0.929 (val), f1: 0.932 (train), 0.929 (val), balanced_accuracy: 0.929 (train), 0.925 (val)\n",
      "Epoch 9/10: Loss: 0.215 (train), 0.238 (val), accuracy: 0.933 (train), 0.926 (val), f1: 0.933 (train), 0.926 (val), balanced_accuracy: 0.930 (train), 0.923 (val)\n",
      "Epoch 10/10: Loss: 0.210 (train), 0.231 (val), accuracy: 0.935 (train), 0.928 (val), f1: 0.935 (train), 0.928 (val), balanced_accuracy: 0.932 (train), 0.924 (val)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(node_features.shape[1], len(torch.unique(data.y)))\n",
    "\n",
    "results = experiment(mlp, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(results, 'mlp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SVMClassifier(node_features.shape[1], len(torch.unique(data.y)))\n",
    "\n",
    "# svm.fit(x_train, y_train)\n",
    "\n",
    "# svm.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg = LogisticRegression(node_features.shape[1], len(torch.unique(data.y)))\n",
    "\n",
    "# log_reg.fit(x_train, y_train)\n",
    "\n",
    "# y_pred = log_reg.predict(x_val)\n",
    "\n",
    "# print(f'Accuracy: {accuracy(y_pred, y_val)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rigged",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
