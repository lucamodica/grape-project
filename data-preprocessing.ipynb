{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Reddit, Amazon\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "import random\n",
    "from torch.functional import F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.loader import NeighborSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Amazon dataset\n",
    "amazon_computers_dataset = Amazon(root='data/Amazon', name='Computers')\n",
    "data = amazon_computers_dataset[0]\n",
    "\n",
    "# split \n",
    "\n",
    "\n",
    "loader = DataLoader(amazon_computers_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data Data(x=[13752, 767], edge_index=[2, 491722], y=[13752])\n",
      "num nodes 13752\n",
      "Num edges 491722\n",
      "num features 767\n",
      "is undirected True\n",
      "is directed False\n"
     ]
    }
   ],
   "source": [
    "# print info about the dataset\n",
    "print(\"data\", data)\n",
    "print(\"num nodes\", data.num_nodes)\n",
    "print(\"Num edges\", data.num_edges)\n",
    "print(\"num features\", data.num_features)\n",
    "print(\"is undirected\", data.is_undirected())\n",
    "print(\"is directed\", data.is_directed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots for data visualization and exploration\n",
    "# G = to_networkx(data, to_undirected=True)\n",
    "# pos = nx.spring_layout(G)\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# nx.draw(G, pos, node_size=10, width=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(data, num_nodes, train_ratio, val_ratio):\n",
    "    # Generate and shuffle indices\n",
    "    indices = list(range(num_nodes))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # Create masks\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    # Set proportions for train, val, and test\n",
    "    train_end = int(train_ratio * num_nodes)\n",
    "    val_end = train_end + int(val_ratio * num_nodes)\n",
    "    \n",
    "    train_mask[indices[:train_end]] = True\n",
    "    val_mask[indices[train_end:val_end]] = True\n",
    "    test_mask[indices[val_end:]] = True\n",
    "\n",
    "    # Assign the custom masks to the dataset\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "    \n",
    "def train_test_split_graph(data, train_ratio, val_ratio):\n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    # Call the function to create masks\n",
    "    create_masks(data, num_nodes, train_ratio, val_ratio)\n",
    "    \n",
    "    # Step 5: Create train, validation, and test node indices based on the masks\n",
    "    train_idx = data.train_mask.nonzero(as_tuple=False).view(-1)\n",
    "    val_idx = data.val_mask.nonzero(as_tuple=False).view(-1)\n",
    "    test_idx = data.test_mask.nonzero(as_tuple=False).view(-1)\n",
    "    \n",
    "    # Create the NeighborSampler objects using node indices for each subset\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_idx, sizes=[15, 10], batch_size=64, shuffle=True, num_workers=4)\n",
    "    val_loader = NeighborSampler(data.edge_index, node_idx=val_idx, sizes=[15, 10], batch_size=64, shuffle=False, num_workers=4)\n",
    "    test_loader = NeighborSampler(data.edge_index, node_idx=test_idx, sizes=[15, 10], batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, data_loader, device, data):\n",
    "    \"\"\"Train the model for one epoch using NeighborSampler mini-batches.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_size, n_id, adjs in data_loader:\n",
    "        # `n_id` includes target nodes and all sampled neighbors in this mini-batch\n",
    "        # `batch_size` is the number of target nodes for which loss should be calculated\n",
    "\n",
    "        adjs = [adj.to(device) for adj in adjs]  # Move sampled adjacency matrices to GPU\n",
    "\n",
    "        # Move input features of all nodes in `n_id` to the device\n",
    "        x_input = data.x[n_id].to(device)\n",
    "        \n",
    "        # Compute the model's predictions for the mini-batch\n",
    "        out = model(x_input, adjs[0].edge_index)\n",
    "        \n",
    "        # Calculate loss only for the first `batch_size` target nodes\n",
    "        loss = F.nll_loss(out[:batch_size], data.y[n_id[:batch_size]].to(device))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, data):\n",
    "    \"\"\"Evaluate the model on the validation or test set using NeighborSampler mini-batches.\"\"\"\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch_size, n_id, adjs in data_loader:\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        \n",
    "        # Compute predictions for all nodes in the mini-batch\n",
    "        x_input = data.x[n_id].to(device)\n",
    "        out = model(x_input, adjs[0].edge_index)\n",
    "\n",
    "        # Get predictions only for the first `batch_size` target nodes\n",
    "        pred = out[:batch_size].max(dim=1)[1]\n",
    "        \n",
    "        # Compare with the actual labels of the target nodes\n",
    "        total_correct += (pred == data.y[n_id[:batch_size]].to(device)).sum().item()\n",
    "\n",
    "    # Calculate accuracy based on the number of target nodes in the entire set\n",
    "    return total_correct / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define a simple GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.4793, Train Acc: 0.7605, Val Acc: 0.7571\n",
      "Epoch: 02, Loss: 0.5610, Train Acc: 0.8431, Val Acc: 0.8284\n",
      "Epoch: 03, Loss: 0.4964, Train Acc: 0.8526, Val Acc: 0.8465\n",
      "Epoch: 04, Loss: 0.4836, Train Acc: 0.8413, Val Acc: 0.8371\n",
      "Epoch: 05, Loss: 0.4643, Train Acc: 0.8554, Val Acc: 0.8378\n",
      "Epoch: 06, Loss: 0.5066, Train Acc: 0.8646, Val Acc: 0.8436\n",
      "Epoch: 07, Loss: 0.4457, Train Acc: 0.8626, Val Acc: 0.8400\n",
      "Epoch: 08, Loss: 0.4503, Train Acc: 0.8702, Val Acc: 0.8516\n",
      "Epoch: 09, Loss: 0.4359, Train Acc: 0.8704, Val Acc: 0.8618\n",
      "Final Test Accuracy: 0.8561\n"
     ]
    }
   ],
   "source": [
    "def main(gnn: torch.nn.Module):\n",
    "\n",
    "    amazon_computers_dataset = Amazon(root='data/Amazon', name='Computers')\n",
    "    data = amazon_computers_dataset[0]\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "\n",
    "    train_ratio = 0.8\n",
    "    val_ratio = 0.1\n",
    "    test_ratio = 0.1\n",
    "\n",
    "    train_loader, val_loader, test_loader = train_test_split_graph(data, train_ratio, val_ratio)\n",
    "\n",
    "    # Initialize the model and optimizer\n",
    "    model = gnn(in_channels=data.num_node_features, hidden_channels=64, out_channels=amazon_computers_dataset.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, 10):\n",
    "        loss = train_epoch(model, optimizer, train_loader, device, data)\n",
    "        train_acc = evaluate(model, train_loader, device, data)\n",
    "        val_acc = evaluate(model, val_loader, device, data)\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Final test accuracy\n",
    "    test_acc = evaluate(model, test_loader, device, data)\n",
    "    print(f'Final Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "\n",
    "# Step 6: Run the main function\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "main(gnn=GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.8295, Train Acc: 0.8684, Val Acc: 0.8480\n",
      "Epoch: 02, Loss: 0.4084, Train Acc: 0.8856, Val Acc: 0.8545\n",
      "Epoch: 03, Loss: 0.3587, Train Acc: 0.8956, Val Acc: 0.8625\n",
      "Epoch: 04, Loss: 0.3492, Train Acc: 0.8824, Val Acc: 0.8531\n",
      "Epoch: 05, Loss: 0.3253, Train Acc: 0.9000, Val Acc: 0.8756\n",
      "Epoch: 06, Loss: 0.3618, Train Acc: 0.8919, Val Acc: 0.8807\n",
      "Epoch: 07, Loss: 0.3210, Train Acc: 0.9049, Val Acc: 0.8778\n",
      "Epoch: 08, Loss: 0.3273, Train Acc: 0.9026, Val Acc: 0.8800\n",
      "Epoch: 09, Loss: 0.3069, Train Acc: 0.9156, Val Acc: 0.8836\n",
      "Final Test Accuracy: 0.8765\n"
     ]
    }
   ],
   "source": [
    "main(gnn=GAT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rigged",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
