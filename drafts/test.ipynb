{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Reddit, Amazon\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "import random\n",
    "from torch.functional import F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.loader import NeighborSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Amazon dataset\n",
    "amazon_computers_dataset = Amazon(root='data/Amazon', name='Computers')\n",
    "data = amazon_computers_dataset[0]\n",
    "\n",
    "# split \n",
    "\n",
    "\n",
    "loader = DataLoader(amazon_computers_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data Data(x=[13752, 767], edge_index=[2, 491722], y=[13752])\n",
      "num nodes 13752\n",
      "Num edges 491722\n",
      "num features 767\n",
      "is undirected True\n",
      "is directed False\n"
     ]
    }
   ],
   "source": [
    "# print info about the dataset\n",
    "print(\"data\", data)\n",
    "print(\"num nodes\", data.num_nodes)\n",
    "print(\"Num edges\", data.num_edges)\n",
    "print(\"num features\", data.num_features)\n",
    "print(\"is undirected\", data.is_undirected())\n",
    "print(\"is directed\", data.is_directed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots for data visualization and exploration\n",
    "# G = to_networkx(data, to_undirected=True)\n",
    "# pos = nx.spring_layout(G)\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# nx.draw(G, pos, node_size=10, width=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(data, num_nodes, train_ratio, val_ratio):\n",
    "    # Generate and shuffle indices\n",
    "    indices = list(range(num_nodes))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # Create masks\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    # Set proportions for train, val, and test\n",
    "    train_end = int(train_ratio * num_nodes)\n",
    "    val_end = train_end + int(val_ratio * num_nodes)\n",
    "    \n",
    "    train_mask[indices[:train_end]] = True\n",
    "    val_mask[indices[train_end:val_end]] = True\n",
    "    test_mask[indices[val_end:]] = True\n",
    "\n",
    "    # Assign the custom masks to the dataset\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "    \n",
    "def train_test_split_graph(data, train_ratio, val_ratio):\n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    # Call the function to create masks\n",
    "    create_masks(data, num_nodes, train_ratio, val_ratio)\n",
    "    \n",
    "    # Step 5: Create train, validation, and test node indices based on the masks\n",
    "    train_idx = data.train_mask.nonzero(as_tuple=False).view(-1)\n",
    "    val_idx = data.val_mask.nonzero(as_tuple=False).view(-1)\n",
    "    test_idx = data.test_mask.nonzero(as_tuple=False).view(-1)\n",
    "    \n",
    "    # Create the NeighborSampler objects using node indices for each subset\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_idx, sizes=[15, 10], batch_size=64, shuffle=True, num_workers=4)\n",
    "    val_loader = NeighborSampler(data.edge_index, node_idx=val_idx, sizes=[15, 10], batch_size=64, shuffle=False, num_workers=4)\n",
    "    test_loader = NeighborSampler(data.edge_index, node_idx=test_idx, sizes=[15, 10], batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, data_loader, device, data):\n",
    "    \"\"\"Train the model for one epoch using NeighborSampler mini-batches.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_size, n_id, adjs in data_loader:\n",
    "        # `n_id` includes target nodes and all sampled neighbors in this mini-batch\n",
    "        # `batch_size` is the number of target nodes for which loss should be calculated\n",
    "\n",
    "        adjs = [adj.to(device) for adj in adjs]  # Move sampled adjacency matrices to GPU\n",
    "\n",
    "        # Move input features of all nodes in `n_id` to the device\n",
    "        x_input = data.x[n_id].to(device)\n",
    "        \n",
    "        # Compute the model's predictions for the mini-batch\n",
    "        out = model(x_input, adjs[0].edge_index)\n",
    "        \n",
    "        # Calculate loss only for the first `batch_size` target nodes\n",
    "        loss = F.nll_loss(out[:batch_size], data.y[n_id[:batch_size]].to(device))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, data):\n",
    "    \"\"\"Evaluate the model on the validation or test set using NeighborSampler mini-batches.\"\"\"\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch_size, n_id, adjs in data_loader:\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        \n",
    "        # Compute predictions for all nodes in the mini-batch\n",
    "        x_input = data.x[n_id].to(device)\n",
    "        out = model(x_input, adjs[0].edge_index)\n",
    "\n",
    "        # Get predictions only for the first `batch_size` target nodes\n",
    "        pred = out[:batch_size].max(dim=1)[1]\n",
    "        \n",
    "        # Compare with the actual labels of the target nodes\n",
    "        total_correct += (pred == data.y[n_id[:batch_size]].to(device)).sum().item()\n",
    "\n",
    "    # Calculate accuracy based on the number of target nodes in the entire set\n",
    "    return total_correct / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define a simple GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_206061/1425343495.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('data/amazon_product_data.pt')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 20 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 35\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Final test accuracy\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# test_acc = evaluate(model, test_loader, device, data)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# print(f'Final Test Accuracy: {test_acc:.4f}')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# if __name__ == \"__main__\":\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     main()\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGCN\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(gnn)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m evaluate(model, train_loader, device, data)\n\u001b[1;32m     23\u001b[0m     val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device, data)\n",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, data_loader, device, data)\u001b[0m\n\u001b[1;32m     16\u001b[0m out \u001b[38;5;241m=\u001b[39m model(x_input, adjs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate loss only for the first `batch_size` target nodes\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn_id\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/mambaforge/envs/rigged/lib/python3.10/site-packages/torch/nn/functional.py:2778\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2777\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 20 is out of bounds."
     ]
    }
   ],
   "source": [
    "def main(gnn: torch.nn.Module):\n",
    "\n",
    "    data = torch.load('data/amazon_product_data.pt')\n",
    "    # data = amazon_computers_dataset[0]s\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "\n",
    "    train_ratio = 0.8\n",
    "    val_ratio = 0.1\n",
    "    test_ratio = 0.1\n",
    "\n",
    "    train_loader, val_loader = train_test_split_graph(data, train_ratio, val_ratio)\n",
    "\n",
    "    # Initialize the model and optimizer\n",
    "    model = gnn(in_channels=data.num_node_features, hidden_channels=64, out_channels=amazon_computers_dataset.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, 10):\n",
    "        loss = train_epoch(model, optimizer, train_loader, device, data)\n",
    "        train_acc = evaluate(model, train_loader, device, data)\n",
    "        val_acc = evaluate(model, val_loader, device, data)\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Final test accuracy\n",
    "    # test_acc = evaluate(model, test_loader, device, data)\n",
    "    # print(f'Final Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "\n",
    "# Step 6: Run the main function\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "main(gnn=GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.6726, Train Acc: 0.8834, Val Acc: 0.8604\n",
      "Epoch: 02, Loss: 0.3990, Train Acc: 0.8955, Val Acc: 0.8778\n",
      "Epoch: 03, Loss: 0.3593, Train Acc: 0.8909, Val Acc: 0.8625\n",
      "Epoch: 04, Loss: 0.3495, Train Acc: 0.9086, Val Acc: 0.8851\n",
      "Epoch: 05, Loss: 0.3315, Train Acc: 0.9033, Val Acc: 0.8822\n",
      "Epoch: 06, Loss: 0.3345, Train Acc: 0.8776, Val Acc: 0.8545\n",
      "Epoch: 07, Loss: 0.3329, Train Acc: 0.8759, Val Acc: 0.8531\n",
      "Epoch: 08, Loss: 0.3452, Train Acc: 0.8876, Val Acc: 0.8516\n",
      "Epoch: 09, Loss: 0.3169, Train Acc: 0.8784, Val Acc: 0.8538\n",
      "Final Test Accuracy: 0.8547\n"
     ]
    }
   ],
   "source": [
    "main(gnn=GAT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rigged",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
